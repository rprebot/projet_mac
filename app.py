import streamlit as st
import streamlit.components.v1 as components
import os
import json
import html
from urllib.parse import urlencode
from openai import OpenAI
from mistralai import Mistral
import requests
from dotenv import load_dotenv

# Charger les variables d'environnement depuis .env
load_dotenv()


def copy_button(text: str, button_id: str):
    """G√©n√®re un bouton HTML/JS pour copier du texte dans le presse-papiers"""
    # √âchapper le texte pour JavaScript
    escaped_text = html.escape(text).replace('\n', '\\n').replace('\r', '').replace("'", "\\'")

    html_code = f"""
    <button id="{button_id}" onclick="copyText_{button_id}()" style="
        background-color: #f0f2f6;
        border: 1px solid #ddd;
        border-radius: 8px;
        padding: 8px 16px;
        cursor: pointer;
        font-size: 14px;
        display: inline-flex;
        align-items: center;
        gap: 6px;
        transition: background-color 0.2s;
    " onmouseover="this.style.backgroundColor='#e0e2e6'" onmouseout="this.style.backgroundColor='#f0f2f6'">
        <span id="icon_{button_id}">üìã</span> <span id="label_{button_id}">Copier la r√©ponse</span>
    </button>
    <script>
        function copyText_{button_id}() {{
            const text = '{escaped_text}';
            const decodedText = text.replace(/&amp;/g, '&').replace(/&lt;/g, '<').replace(/&gt;/g, '>').replace(/&quot;/g, '"').replace(/&#x27;/g, "'");
            navigator.clipboard.writeText(decodedText).then(function() {{
                document.getElementById('icon_{button_id}').innerText = '‚úÖ';
                document.getElementById('label_{button_id}').innerText = 'Copi√© !';
                setTimeout(function() {{
                    document.getElementById('icon_{button_id}').innerText = 'üìã';
                    document.getElementById('label_{button_id}').innerText = 'Copier la r√©ponse';
                }}, 2000);
            }});
        }}
    </script>
    """
    components.html(html_code, height=50)


# Configuration de la page
st.set_page_config(page_title="Assistant Juridique IA", layout="wide")

# Titre de l'application
st.title("Assistant Juridique IA - R√©sum√© de conclusion et r√©daction de l'expos√© du litige")

# Mapping des noms de prompts vers les fichiers
PROMPT_FILES = {
    "R√©sum√© Conclusions": "prompts/resume_conclusions.md",
    "Synth√®se Faits & Proc√©dure": "prompts/synthese_faits_procedure.md",
    "Synth√®se Moyens": "prompts/synthese_moyens.md",
    "Rapport de synth√®se": "prompts/synthese_faits_procedure_moyens.md",
    "R√©daction Expos√© du Litige": "prompts/redaction_expose_litige.md"
}

# Prompts cha√Æn√©s (2 √©tapes)
CHAINED_PROMPTS = {
    "R√©sum√© Conclusions (2 √©tapes)": {
        "etape1": "prompts_chaines/resume_conclusions_etape1_structure.md",
        "etape2": "prompts_chaines/resume_conclusions_etape2_resume.md"
    }
}

def load_chained_prompts():
    """
    Charge les prompts cha√Æn√©s (2 √©tapes) depuis les fichiers markdown
    """
    prompts = {}
    for name, files in CHAINED_PROMPTS.items():
        prompts[name] = {}
        for etape, filename in files.items():
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    prompts[name][etape] = f.read()
            except FileNotFoundError:
                prompts[name][etape] = f"Erreur : Le fichier {filename} n'a pas √©t√© trouv√©."
    return prompts

# Charger les prompts cha√Æn√©s
SYSTEM_PROMPTS_CHAINED = load_chained_prompts()

# Charger les prompts syst√®me depuis les fichiers .md
def load_system_prompts():
    """
    Charge les prompts syst√®me depuis les fichiers markdown
    """
    prompts = {}

    # Chemins des fichiers de prompts (ordre pr√©serv√© avec Python 3.7+)
    prompt_files = [
        ("R√©sum√© Conclusions", "prompts/resume_conclusions.md"),
        ("Synth√®se Faits & Proc√©dure", "prompts/synthese_faits_procedure.md"),
        ("Synth√®se Moyens", "prompts/synthese_moyens.md"),
        ("Rapport de synth√®se", "prompts/synthese_faits_procedure_moyens.md"),
        ("R√©daction Expos√© du Litige", "prompts/redaction_expose_litige.md")
    ]

    # Charger chaque fichier
    for name, filename in prompt_files:
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                prompts[name] = f.read()
        except FileNotFoundError:
            prompts[name] = f"Erreur : Le fichier {filename} n'a pas √©t√© trouv√©."

    return prompts

# Charger les fichiers de conclusions
def load_conclusion_files():
    """
    Charge les fichiers de conclusions juridiques
    """
    files = {}

    conclusion_files = {
        "Dossier 4 - Conclusion Appelante": "dossiers/Dossier_4_conclusion_appelante.txt",
        "Dossier 4 - Conclusion Intim√©e": "dossiers/Dossier_4_conclusion_intimee.txt",
        "Dossier 5 - Leonard (Employeur)": "dossiers/Dossier_5_Leonard_(employeur).txt",
        "Dossier 5 - Leonard (Salari√©)": "dossiers/Dossier_5_Leonard_(salarie).txt",
        "Dossier 6 - Conclusion Appelant": "dossiers/Dossier_6_conclusion_appelant.txt",
        "Dossier 6 - Conclusion Intim√©e": "dossiers/Dossier_6_conclusion_intimee.txt",
        "Dossier 8 - Demandeur": "dossiers/Dossier_8_demandeur.txt",
        "Dossier 8 - Intim√©e": "dossiers/Dossier_8_intimee.txt",
        "Dossier 13 - Conclusion D√©fendeur": "dossiers/dossier_13_conclusion_defendeur.txt",
        "Dossier 15 - D√©fendeur": "dossiers/Dossier_15_defendeur.txt",
        "Dossier 15 - Demandeur": "dossiers/Dossier_15_demandeur.txt"
    }

    for name, filename in conclusion_files.items():
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                files[name] = f.read()
        except FileNotFoundError:
            files[name] = f"Erreur : Le fichier {filename} n'a pas √©t√© trouv√©."

    return files

# Charger les prompts syst√®me
SYSTEM_PROMPTS = load_system_prompts()

# Charger les fichiers de conclusions
CONCLUSION_FILES = load_conclusion_files()

# Initialiser session_state pour l'historique
if "messages" not in st.session_state:
    st.session_state.messages = []

if "message_count" not in st.session_state:
    st.session_state.message_count = 0

if "evaluations" not in st.session_state:
    st.session_state.evaluations = {}  # Cl√© = index du message assistant

if "custom_prompt" not in st.session_state:
    st.session_state.custom_prompt = "Vous √™tes un assistant juridique. R√©pondez aux questions de l'utilisateur de mani√®re pr√©cise et professionnelle."

if "custom_trame" not in st.session_state:
    st.session_state.custom_trame = """[TRAME √Ä COMPL√âTER]

Renseignez ici la structure de l'expos√© du litige que vous souhaitez obtenir.

Exemple :
## I. EXPOS√â DU LITIGE
### A. Les faits
[Consignes pour cette section...]

### B. La proc√©dure
[Consignes pour cette section...]

### C. Les pr√©tentions des parties
[Consignes pour cette section...]

### D. Les moyens des parties
[Consignes pour cette section...]
"""

# Cl√©s API depuis les variables d'environnement
NEBIUS_API_KEY = os.getenv("NEBIUS_API_KEY", "")
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY", "")
ALBERT_API_KEY = os.getenv("ALBERT_API_KEY", "")

# Limites de tokens par mod√®le (contexte d'entr√©e)
MODEL_TOKEN_LIMITS = {
    "Albert Large": 128000,
    "Mixtral 8x22B (Mistral)": 64000,
    "Mistral-medium-2508": 128000,
    "GPT-OSS-120B (Nebius)": 128000
}

def estimate_tokens(text):
    """
    Estime le nombre de tokens dans un texte.
    Approximation : 1 token ‚âà 4 caract√®res pour le fran√ßais.
    """
    if not text:
        return 0
    return len(text) // 4

def count_messages_tokens(system_prompt, messages_history):
    """
    Compte le nombre total de tokens dans les messages.
    """
    total = estimate_tokens(system_prompt)
    for msg in messages_history:
        total += estimate_tokens(msg.get("content", ""))
    return total

def load_evaluation_criteria():
    """
    Charge les crit√®res d'√©valuation depuis le fichier evaluation_criteria.json
    """
    try:
        with open("evaluation_criteria.json", 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return {}

# Charger les crit√®res d'√©valuation
EVALUATION_CRITERIA = load_evaluation_criteria()

def load_evaluation_prompt():
    """
    Charge le prompt d'√©valuation depuis le fichier evaluation_prompt.md
    """
    try:
        with open("prompts/evaluation_prompt.md", 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        return None

def evaluate_with_magistral(document_source, reponse_llm, prompt_type):
    """
    √âvalue la r√©ponse du LLM avec Magistral Medium (mod√®le de raisonnement).
    Retourne un dictionnaire avec les scores et le raisonnement.
    """
    if not MISTRAL_API_KEY:
        return {"error": "Cl√© API Mistral non configur√©e"}

    # Charger le template du prompt
    prompt_template = load_evaluation_prompt()
    if not prompt_template:
        return {"error": "Fichier evaluation_prompt.md non trouv√©"}

    criteria = EVALUATION_CRITERIA.get(prompt_type, EVALUATION_CRITERIA["R√©sum√© Conclusions"])

    # Construire la liste des crit√®res pour le prompt
    criteres_text = "\n".join([f"- **{nom}** : {description}" for nom, description in criteria["criteres"]])
    criteres_json = ", ".join([f'"{nom}": <note 1-5>' for nom, _ in criteria["criteres"]])

    # Formater le prompt avec les variables
    evaluation_prompt = prompt_template.format(
        task_description=criteria["description"],
        document_source=document_source[:15000],
        reponse_llm=reponse_llm,
        criteres_text=criteres_text,
        criteres_json=criteres_json
    )

    try:
        client = Mistral(api_key=MISTRAL_API_KEY)
        response = client.chat.complete(
            model="magistral-medium-2506",
            messages=[{"role": "user", "content": evaluation_prompt}]
        )

        message = response.choices[0].message
        message_content = message.content

        # Magistral retourne une liste d'objets (ThinkChunk, TextChunk)
        response_text = ""
        reasoning_text = ""

        if isinstance(message_content, list):
            for item in message_content:
                # Acc√©der aux attributs des objets Mistral
                item_type = getattr(item, 'type', None)

                if item_type == 'thinking':
                    # ThinkChunk contient le raisonnement
                    thinking_content = getattr(item, 'thinking', [])
                    for think_item in thinking_content:
                        if hasattr(think_item, 'text'):
                            reasoning_text += think_item.text

                elif item_type == 'text':
                    # TextChunk contient la r√©ponse finale
                    if hasattr(item, 'text'):
                        response_text += item.text

                # Fallback : essayer d'acc√©der directement √† 'text'
                elif hasattr(item, 'text'):
                    response_text += item.text

        elif isinstance(message_content, str):
            response_text = message_content

        else:
            return {"error": "Format de contenu non reconnu", "raw_response": str(type(message_content))}

        # Si pas de r√©ponse texte, utiliser le raisonnement
        if not response_text and reasoning_text:
            response_text = reasoning_text

        if not response_text:
            return {"error": "R√©ponse vide de Magistral", "raw_response": str(message_content)}

        # Garder la trace compl√®te (raisonnement + r√©ponse)
        full_response = f"[RAISONNEMENT]\n{reasoning_text}\n\n[R√âPONSE]\n{response_text}"

        # Nettoyer les backticks markdown si pr√©sents (```json ... ```)
        import re
        if "```json" in response_text:
            match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
            if match:
                response_text = match.group(1)
        elif "```" in response_text:
            match = re.search(r'```\s*(.*?)\s*```', response_text, re.DOTALL)
            if match:
                response_text = match.group(1)

        # Extraire le JSON de la r√©ponse
        json_start = response_text.find('{')
        json_end = response_text.rfind('}') + 1

        if json_start != -1 and json_end > json_start:
            json_str = response_text[json_start:json_end]
            try:
                evaluation = json.loads(json_str)
                evaluation["reasoning_trace"] = full_response
                return evaluation
            except json.JSONDecodeError:
                json_str_clean = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', json_str)
                evaluation = json.loads(json_str_clean)
                evaluation["reasoning_trace"] = full_response
                return evaluation
        else:
            return {"error": "Format de r√©ponse invalide - Pas de JSON trouv√©", "raw_response": response_text}

    except json.JSONDecodeError as e:
        return {"error": f"Erreur parsing JSON: {str(e)}", "raw_response": str(response_text) if 'response_text' in dir() else "N/A"}
    except Exception as e:
        return {"error": f"Erreur √©valuation: {str(e)}", "details": str(type(e).__name__)}

# Sidebar - Bouton Nouvelle conversation en haut
if st.sidebar.button("üîÑ Nouvelle conversation", type="primary", use_container_width=True):
    st.session_state.messages = []
    st.session_state.message_count = 0
    st.rerun()

st.sidebar.markdown("---")

# Sidebar - Configuration
st.sidebar.header("Configuration")

# S√©lection du mod√®le
model_choice = st.sidebar.selectbox(
    "Mod√®le LLM",
    ["Albert Large", "Mixtral 8x22B (Mistral)", "Mistral-medium-2508", "GPT-OSS-120B (Nebius)"]
)

# S√©lection du prompt syst√®me (incluant le prompt personnalisable)
# Note: les prompts cha√Æn√©s (2 √©tapes) sont d√©sactiv√©s pour l'instant mais le code reste disponible
prompt_options = list(SYSTEM_PROMPTS.keys()) + ["Prompt personnalisable"]
prompt_choice = st.sidebar.selectbox(
    "Prompt syst√®me",
    prompt_options
)

# Indicateur si le prompt s√©lectionn√© est un prompt cha√Æn√©
is_chained_prompt = prompt_choice in CHAINED_PROMPTS

# R√©cup√©rer le prompt syst√®me s√©lectionn√©
if prompt_choice == "Prompt personnalisable":
    system_prompt = st.session_state.custom_prompt
elif is_chained_prompt:
    # Pour les prompts cha√Æn√©s, on utilise le prompt de l'√©tape 1 comme r√©f√©rence
    system_prompt = SYSTEM_PROMPTS_CHAINED[prompt_choice]["etape1"]
elif prompt_choice == "R√©daction Expos√© du Litige":
    # Ins√©rer automatiquement la trame de l'utilisateur dans le prompt
    base_prompt = SYSTEM_PROMPTS[prompt_choice]
    # Remplacer le placeholder par la trame de l'utilisateur
    trame_placeholder = """```
[TRAME √Ä COMPL√âTER PAR L'UTILISATEUR]

Ins√©rez ici la structure et les consignes sp√©cifiques pour la r√©daction de l'expos√© du litige.

Exemple de trame possible :
- Section 1 : [Titre et consignes]
- Section 2 : [Titre et consignes]
- Section 3 : [Titre et consignes]
- ...

```"""
    system_prompt = base_prompt.replace(trame_placeholder, f"```\n{st.session_state.custom_trame}\n```")
else:
    system_prompt = SYSTEM_PROMPTS[prompt_choice]

# √âditeur du prompt s√©lectionn√© (sauf pour le prompt personnalisable qui a son propre onglet)
if prompt_choice != "Prompt personnalisable":
    with st.sidebar.expander("‚úèÔ∏è √âditer le prompt"):
        edited_prompt = st.text_area(
            "Contenu du prompt",
            value=system_prompt,
            height=250,
            key=f"sidebar_prompt_editor_{prompt_choice}",
            label_visibility="collapsed"
        )
        if st.button("üíæ Sauvegarder", use_container_width=True):
            filename = PROMPT_FILES.get(prompt_choice)
            if filename:
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(edited_prompt)
                st.success("‚úÖ Sauvegard√© !")
                st.rerun()
else:
    st.sidebar.info("‚úèÔ∏è √âditez votre prompt dans l'onglet 'Prompt personnalisable'")

# Option d'√©valuation automatique
st.sidebar.markdown("---")
st.sidebar.header("√âvaluation LLM")
enable_evaluation = st.sidebar.checkbox(
    "Activer l'√©valuation Magistral",
    value=False,
    key="enable_magistral_evaluation",
    help="√âvalue automatiquement chaque r√©ponse avec Magistral Medium (mod√®le de raisonnement)"
)

# Fonction pour appeler le mod√®le
def call_model(model_choice, system_prompt, messages_history):
    """
    Appelle le mod√®le s√©lectionn√© avec l'historique des messages
    """
    # Construire les messages avec le syst√®me + historique
    full_messages = [{"role": "system", "content": system_prompt}] + messages_history

    # Albert Large
    if model_choice == "Albert Large":
        if not ALBERT_API_KEY:
            raise ValueError("La cl√© API Albert n'est pas configur√©e.")

        headers = {
            "Authorization": f"Bearer {ALBERT_API_KEY}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": "albert-large",
            "messages": full_messages,
            "temperature": 0.3
        }
        # URL √† adapter selon la documentation de l'API Albert
        api_url = "https://albert.api.etalab.gouv.fr/v1/chat/completions"
        response = requests.post(api_url, json=payload, headers=headers)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]

    # Mixtral 8x22B via Mistral
    elif model_choice == "Mixtral 8x22B (Mistral)":
        if not MISTRAL_API_KEY:
            raise ValueError("La cl√© API Mistral n'est pas configur√©e.")

        client = Mistral(api_key=MISTRAL_API_KEY)
        response = client.chat.complete(
            model="mistral-large-latest",
            messages=full_messages,
            temperature=0.3
        )
        return response.choices[0].message.content

    # Mistral Medium 2508 (mod√®le assistant num√©rique)
    elif model_choice == "Mistral-medium-2508":
        if not MISTRAL_API_KEY:
            raise ValueError("La cl√© API Mistral n'est pas configur√©e.")

        client = Mistral(api_key=MISTRAL_API_KEY)
        response = client.chat.complete(
            model="mistral-medium-2508",
            messages=full_messages,
            temperature=0.3
        )
        return response.choices[0].message.content

    # GPT-OSS-120B via Nebius (OpenAI compatible) avec reasoning
    elif model_choice == "GPT-OSS-120B (Nebius)":
        if not NEBIUS_API_KEY:
            raise ValueError("La cl√© API Nebius n'est pas configur√©e.")

        client = OpenAI(
            base_url="https://api.studio.nebius.ai/v1/",
            api_key=NEBIUS_API_KEY
        )
        response = client.chat.completions.create(
            model="openai/gpt-oss-120b",
            messages=full_messages,
            temperature=0.3,
            extra_body={
                "reasoning": {
                    "effort": "high"
                }
            }
        )
        return response.choices[0].message.content


def call_model_chained(model_choice, prompt_choice, user_query):
    """
    Appelle le mod√®le en 2 √©tapes pour les prompts cha√Æn√©s.
    √âtape 1 : Extraction de la structure
    √âtape 2 : R√©sum√© avec la structure fournie
    Retourne un dictionnaire avec les r√©sultats des 2 √©tapes.
    """
    prompts = SYSTEM_PROMPTS_CHAINED[prompt_choice]

    # √âTAPE 1 : Extraction de la structure
    prompt_etape1 = prompts["etape1"]
    messages_etape1 = [{"role": "user", "content": user_query}]

    structure_extraite = call_model(model_choice, prompt_etape1, messages_etape1)

    # √âTAPE 2 : R√©sum√© avec la structure fournie
    prompt_etape2 = prompts["etape2"]

    # Construire le message pour l'√©tape 2 avec le document ET la structure
    message_etape2 = f"""## DOCUMENT SOURCE (Conclusion juridique)

{user_query}

---

## STRUCTURE EXTRAITE (√Ä suivre imp√©rativement)

{structure_extraite}
"""

    messages_etape2 = [{"role": "user", "content": message_etape2}]

    resume_final = call_model(model_choice, prompt_etape2, messages_etape2)

    return {
        "etape1_structure": structure_extraite,
        "etape2_resume": resume_final
    }


# Cr√©er les onglets
tab1, tab2, tab3, tab4, tab5 = st.tabs(["üí¨ Chat", "üìÑ Fichiers de conclusions", "‚úèÔ∏è Prompt personnalisable", "üìê Mod√®le de trame", "üìñ Guide d'utilisation"])

# ============================================================
# ONGLET 1 : CHAT
# ============================================================
with tab1:
    st.header("Chat")

    # Afficher l'historique des messages
    for idx, message in enumerate(st.session_state.messages):
        with st.chat_message(message["role"]):
            content = message["content"]

            # Pour les messages utilisateur : tronquer si > 1500 caract√®res
            if message["role"] == "user" and len(content) > 1500:
                st.markdown(f"*[Query - {len(content):,} caract√®res]*")
                with st.expander("üìÑ Voir la query compl√®te"):
                    st.code(content, language=None)
            # Pour les messages assistant : tronquer si > 10000 caract√®res
            elif message["role"] == "assistant" and len(content) > 10000:
                st.markdown(f"{content[:500]}...\n\n*[R√©ponse tronqu√©e - {len(content):,} caract√®res au total]*")
                with st.expander("üìÑ Voir la r√©ponse compl√®te"):
                    st.markdown(content)
            else:
                st.markdown(content)

        # Afficher le lien vers le formulaire Tally apr√®s chaque r√©ponse de l'assistant
        if message["role"] == "assistant":
            # R√©cup√©rer la question de l'utilisateur (message pr√©c√©dent)
            user_question = ""
            if idx > 0 and st.session_state.messages[idx - 1]["role"] == "user":
                user_question = st.session_state.messages[idx - 1]["content"]

            # Construire l'URL Tally avec les hidden fields (taille r√©duite pour √©viter les probl√®mes d'URL)
            tally_params = {
                "Question": user_question[:500],
                "Answer": content[:500],
                "Prompt": prompt_choice,
                "LLMmodel": model_choice
            }
            tally_url = f"https://tally.so/r/9qZx9X?{urlencode(tally_params)}"

            # Boutons d'action : Copier + Feedback
            col_copy, col_feedback = st.columns([1, 1])
            with col_copy:
                copy_button(content, f"copy_btn_{idx}")
            with col_feedback:
                st.link_button("üìù Donner votre avis", tally_url, type="secondary", use_container_width=True)

            # Afficher l'√©valuation si disponible
            if idx in st.session_state.evaluations:
                eval_data = st.session_state.evaluations[idx]
                if "error" not in eval_data:
                    with st.expander(f"üéØ √âvaluation Magistral - Score global : {eval_data.get('score_global', 'N/A')}/5"):
                        # Scores par crit√®re
                        st.markdown("**Scores par crit√®re :**")
                        scores = eval_data.get("scores", {})
                        raisonnements = eval_data.get("raisonnements", {})

                        for critere, score in scores.items():
                            col1, col2 = st.columns([1, 3])
                            with col1:
                                # Couleur selon le score
                                if score >= 4:
                                    st.success(f"{critere}: {score}/5")
                                elif score >= 3:
                                    st.warning(f"{critere}: {score}/5")
                                else:
                                    st.error(f"{critere}: {score}/5")
                            with col2:
                                st.caption(raisonnements.get(critere, ""))

                        st.markdown("---")

                        # Points forts et am√©liorations
                        col_forts, col_amelio = st.columns(2)
                        with col_forts:
                            st.markdown("**‚úÖ Points forts :**")
                            for point in eval_data.get("points_forts", []):
                                st.markdown(f"- {point}")
                        with col_amelio:
                            st.markdown("**‚ö†Ô∏è Points √† am√©liorer :**")
                            for point in eval_data.get("points_amelioration", []):
                                st.markdown(f"- {point}")

                        # Synth√®se
                        st.markdown("---")
                        st.markdown(f"**Synth√®se :** {eval_data.get('synthese', '')}")

                        # Trace de raisonnement (optionnel) - checkbox au lieu d'expander imbriqu√©
                        if "reasoning_trace" in eval_data:
                            if st.checkbox("üîç Voir le raisonnement complet", key=f"reasoning_{idx}"):
                                st.code(eval_data["reasoning_trace"], language=None)
                else:
                    st.warning(f"√âvaluation √©chou√©e : {eval_data.get('error', 'Erreur inconnue')}")
                    # Afficher la r√©ponse brute pour debug - checkbox au lieu d'expander imbriqu√©
                    if "raw_response" in eval_data:
                        if st.checkbox("üîç Voir la r√©ponse brute de Magistral", key=f"raw_{idx}"):
                            st.code(str(eval_data["raw_response"]), language=None)

    # Zone de saisie - d√©sactiv√©e si on a d√©j√† 5 questions pos√©es
    max_questions = 5
    can_ask = st.session_state.message_count < max_questions

    if not can_ask:
        st.info(f"Limite atteinte : vous avez pos√© {max_questions} questions (question initiale + 4 relances). Cliquez sur 'Nouvelle conversation' dans la barre lat√©rale pour recommencer.")

    # Zone de saisie de la question
    user_query = st.chat_input(
        "Votre question..." if can_ask else "Limite de questions atteinte",
        disabled=not can_ask
    )

    # Traiter la question de l'utilisateur
    if user_query and can_ask:
        # Ajouter la question de l'utilisateur √† l'historique
        st.session_state.messages.append({"role": "user", "content": user_query})
        st.session_state.message_count += 1

        # V√©rifier le nombre de tokens avant l'appel
        estimated_tokens = count_messages_tokens(system_prompt, st.session_state.messages)
        token_limit = MODEL_TOKEN_LIMITS.get(model_choice, 32000)

        if estimated_tokens > token_limit:
            st.error(f"""
            ‚ö†Ô∏è **Limite de tokens d√©pass√©e**

            - Tokens estim√©s : **{estimated_tokens:,}** tokens
            - Limite du mod√®le ({model_choice}) : **{token_limit:,}** tokens
            - D√©passement : **{estimated_tokens - token_limit:,}** tokens

            Veuillez r√©duire la taille de votre texte ou d√©marrer une nouvelle conversation.
            """)
            st.session_state.messages.pop()
            st.session_state.message_count -= 1
        else:
            # Afficher info tokens dans la sidebar
            st.sidebar.info(f"üìä Tokens estim√©s : {estimated_tokens:,} / {token_limit:,}")

            # G√©n√©rer et afficher la r√©ponse
            with st.chat_message("assistant"):
                try:
                    # V√©rifier si c'est un prompt cha√Æn√© (2 √©tapes)
                    if is_chained_prompt:
                        # Appel cha√Æn√© en 2 √©tapes
                        with st.spinner("Analyse en 2 √©tapes en cours..."):
                            result = call_model_chained(model_choice, prompt_choice, user_query)

                        # Afficher uniquement le r√©sultat de l'√©tape 2
                        response_text = result["etape2_resume"]
                    else:
                        # Appel simple (1 √©tape)
                        with st.spinner("G√©n√©ration de la r√©ponse..."):
                            response_text = call_model(
                                model_choice,
                                system_prompt,
                                st.session_state.messages
                            )

                    # Ajouter la r√©ponse √† l'historique
                    st.session_state.messages.append({"role": "assistant", "content": response_text})

                    # √âvaluation automatique si activ√©e
                    if enable_evaluation:
                        with st.spinner("√âvaluation avec Magistral Medium..."):
                            # R√©cup√©rer le document source (question de l'utilisateur)
                            document_source = user_query
                            eval_result = evaluate_with_magistral(
                                document_source,
                                response_text,
                                prompt_choice
                            )
                            # Stocker l'√©valuation avec l'index du message
                            eval_index = len(st.session_state.messages) - 1
                            st.session_state.evaluations[eval_index] = eval_result

                    st.rerun()

                except Exception as e:
                    error_msg = f"Erreur lors de la g√©n√©ration de la r√©ponse : {str(e)}"
                    st.error(error_msg)
                    st.session_state.messages.pop()
                    st.session_state.message_count -= 1

# ============================================================
# ONGLET 2 : FICHIERS DE CONCLUSIONS
# ============================================================
with tab2:
    st.markdown("Cliquez sur l'ic√¥ne üìã en haut √† droite de chaque bloc pour copier le contenu.")
    for name, content in CONCLUSION_FILES.items():
        with st.expander(name):
            st.code(content, language=None, line_numbers=False)

# ============================================================
# ONGLET 3 : PROMPT PERSONNALISABLE
# ============================================================
with tab3:
    st.header("Prompt personnalisable")
    st.markdown("""
    Cr√©ez votre propre prompt syst√®me pour personnaliser le comportement de l'assistant.
    Une fois sauvegard√©, s√©lectionnez **"Prompt personnalisable"** dans la liste des prompts syst√®me de la barre lat√©rale.
    """)

    # Zone d'√©dition du prompt personnalis√©
    new_custom_prompt = st.text_area(
        "Votre prompt personnalis√©",
        value=st.session_state.custom_prompt,
        height=400,
        key="custom_prompt_editor",
        help="D√©crivez le comportement souhait√© de l'assistant. Par exemple : 'Vous √™tes un expert en droit du travail...'"
    )

    col1, col2 = st.columns([1, 4])
    with col1:
        if st.button("üíæ Sauvegarder", type="primary", use_container_width=True):
            st.session_state.custom_prompt = new_custom_prompt
            st.success("‚úÖ Prompt personnalis√© sauvegard√© !")
            st.rerun()
    with col2:
        if st.button("üîÑ R√©initialiser", use_container_width=True):
            st.session_state.custom_prompt = "Vous √™tes un assistant juridique. R√©pondez aux questions de l'utilisateur de mani√®re pr√©cise et professionnelle."
            st.success("‚úÖ Prompt r√©initialis√© !")
            st.rerun()

    # Afficher un aper√ßu
    with st.expander("üìã Aper√ßu du prompt actuel"):
        st.code(st.session_state.custom_prompt, language=None)

    # Compteur de caract√®res
    st.caption(f"üìä {len(st.session_state.custom_prompt):,} caract√®res | ~{len(st.session_state.custom_prompt) // 4:,} tokens estim√©s")

# ============================================================
# ONGLET 4 : MOD√àLE DE TRAME
# ============================================================
with tab4:
    st.header("Mod√®le de trame")
    st.markdown("""
    D√©finissez ici la **trame** (structure et ordre des sections) pour la r√©daction de l'expos√© du litige.

    Cette trame sera **automatiquement ins√©r√©e** dans le prompt lorsque vous s√©lectionnerez
    **"R√©daction Expos√© du Litige"** dans la liste des prompts syst√®me.
    """)

    # Avertissement si le prompt n'est pas s√©lectionn√©
    if prompt_choice != "R√©daction Expos√© du Litige":
        st.info("üí° Pour utiliser cette trame, s√©lectionnez **\"R√©daction Expos√© du Litige\"** dans la liste des prompts syst√®me de la barre lat√©rale.")

    # Zone d'√©dition de la trame
    new_custom_trame = st.text_area(
        "Votre trame personnalis√©e",
        value=st.session_state.custom_trame,
        height=450,
        key="custom_trame_editor",
        help="D√©finissez la structure de l'expos√© du litige : titres des sections, ordre, et consignes sp√©cifiques pour chaque partie."
    )

    col1, col2 = st.columns([1, 4])
    with col1:
        if st.button("üíæ Sauvegarder", type="primary", use_container_width=True, key="save_trame"):
            st.session_state.custom_trame = new_custom_trame
            st.success("‚úÖ Trame sauvegard√©e !")
            st.rerun()
    with col2:
        if st.button("üîÑ R√©initialiser", use_container_width=True, key="reset_trame"):
            st.session_state.custom_trame = """[TRAME √Ä COMPL√âTER]

Renseignez ici la structure de l'expos√© du litige que vous souhaitez obtenir.

Exemple :
## I. EXPOS√â DU LITIGE
### A. Les faits
[Consignes pour cette section...]

### B. La proc√©dure
[Consignes pour cette section...]

### C. Les pr√©tentions des parties
[Consignes pour cette section...]

### D. Les moyens des parties
[Consignes pour cette section...]
"""
            st.success("‚úÖ Trame r√©initialis√©e !")
            st.rerun()

    # Afficher un aper√ßu
    with st.expander("üìã Aper√ßu de la trame actuelle"):
        st.markdown(st.session_state.custom_trame)

    # Compteur de caract√®res
    st.caption(f"üìä {len(st.session_state.custom_trame):,} caract√®res | ~{len(st.session_state.custom_trame) // 4:,} tokens estim√©s")

# ============================================================
# ONGLET 5 : GUIDE D'UTILISATION
# ============================================================
with tab5:
    st.header("Guide d'utilisation")
    st.markdown("""
### Mode d'emploi de Streamlit : comment tester les LLM

**Utiliser exclusivement les conclusions anonymis√©es pour les tests**

#### √âtapes pour tester :

1. **Choisir un mod√®le et un prompt** dans la barre lat√©rale

2. **Faire une phrase introductive**, par exemple :
   > *"Peux-tu r√©sumer les conclusions jointes en respectant les consignes du prompt ? Voici les conclusions de l'appelante"*

3. **Coller les conclusions anonymis√©es** dans la zone de saisie

4. **Copier le r√©sultat obtenu** dans un document Word qui sera intitul√© selon les consignes ci-dessous

5. **Analyser le r√©sultat** en comparant avec les conclusions :
   - Vous pouvez faire des commentaires sur le document Word en soulignant les erreurs, les interpr√©tations, les approximations juridiques, etc.
   - Pour effectuer la comparaison, ouvrir les conclusions non anonymis√©es (c'est plus facile)
   - Ne pas trop s'attarder sur la conformit√© des pr√©tentions : un autre syst√®me que l'IA pourra les reprendre in extenso

6. **R√©pondre au questionnaire** (bouton "Donner votre avis" apr√®s chaque r√©ponse)

7. **Poursuivre la conversation** si n√©cessaire en demandant √† l'IA d'am√©liorer sa r√©ponse, puis r√©pondre √† nouveau au questionnaire

8. **Ajouter des compl√©ments sur Notion** dans la cellule r√©serv√©e si vous avez des nouveaux commentaires

9. **Glisser le document Word sur Notion**

---

### R√©p√©ter l'op√©ration

Pour les autres conclusions et prompts, cliquer sur **"Nouvelle conversation"** √† chaque fois.

üí° **Conseil** : Il est plus efficace de tester pour chaque prompt les diff√©rents LLM. Cela permet aussi de copier-coller la question pos√©e.

---

### Intitul√© des documents Word

Les r√©sultats obtenus seront copi√©s dans un document Word intitul√© selon ce format :

```
[N¬∞ Dossier] [NOM DU PROMPT] [appelant ou intim√©] [Initiales]
```

**Exemples :**
- `6 synth√®se faits proced pr√©tention`
- `6 synth√®se des moyens`
- `6 expos√© du litige avec sans trame`

---

### Mode d'emploi de Notion

- Chaque testeur dispose d'une vue avec son nom ‚Üí cliquer dessus et remplir les cellules
- **Pour glisser un document** :
  1. Cliquer sur le bouton avec 6 points
  2. Puis **OUVRIR** dans aper√ßu lat√©ral
  3. Cliquer sur **Ajouter un commentaire**
  4. Puis sur le **trombone** üìé
  5. S√©lectionner votre fichier

üìù *L'enregistrement est automatique*
    """)

# Information sur les cl√©s API dans la sidebar
st.sidebar.markdown("---")
with st.sidebar.expander("‚ÑπÔ∏è Configuration des cl√©s API"):
    st.write("""
    Pour utiliser cette application, vous devez configurer les cl√©s API suivantes
    dans vos variables d'environnement :

    - `ALBERT_API_KEY` pour Albert Large
    - `MISTRAL_API_KEY` pour Mixtral 8x22B
    - `NEBIUS_API_KEY` pour GPT-OSS-120B

    Vous pouvez les d√©finir dans un fichier `.env` √† la racine du projet.
    """)
